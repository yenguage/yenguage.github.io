---
layout: single
title: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, NAACL 2019"
use_math: true
toc: true
toc_sticky: true
comments: true
categories:
- Natural Language Processing
---

[Paper Review] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, NAACL 2019

## Goal
Propose powerful pretrained LM for general language representation 

## Challenge
- unidirectional LM restrict the power of pre-trained representations
conditional objectives are not for bidirectional approach

## Solutions
bidirectional LM 

## Method
- transformer encoder as LM
- pre-train by MLM, NSP

## Evaluation:
- performance evaluation on GLUE(General Language Understanding Evaluation), SQuAD(Stanford Question Answering Dataset) and SWAG(Situations With Adversarial Generations)
- Effect of Pre-training tasks (~ Ablation) , Model Size
- Compare with feature-based approach on NER task

![](https://images.velog.io/images/yenguage/post/9b5f63b6-1840-4443-9a91-1032576c498c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-12-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2010.39.17.png)
![](https://images.velog.io/images/yenguage/post/2397e98a-c748-4f06-aa42-ec5a88fa0237/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-12-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2010.39.56.png)

## Etc.
- decoder는 내부에 masked attention(subsequent positions을 attenting 할 수 없도록 masking ex. t스텝에 t+1스텝 이후의 포지션들을 못 보도록 masking)을 해서 BERT가 원하는 방식대로 MLM을 할 수 없어서 encoder를 사용한 것 같다

